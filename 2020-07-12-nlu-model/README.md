# 自然语言处理的几种模型范式

## 序列到序列任务（Sequence-to-Sequence）

序列到序列任务的大兴和机器翻译有极大的关系，后续的很多任务如语义解析（Semantic Parsing）、语音识别（ASR）、语音合成（TTS）、开放域对话机器人（或聊天机器人，Chatbot，open-domain dialogue system），等等任务，都有这部分的影子。

我们可以抽象的任务，序列到序列是一个不定长的输入序列，到一个不定长的输出序列的变换。典型就是机器翻译的变换，因为输入语言句子和输出语言句子的长度都不确定，这使得系统很难做到一对一映射（如果做到了就是序列标注任务）。

这个时候，我们选择使用一个编码器（Encoder）将输入序列编码为一个固定长度的隐含向量，再将这个向量作为上下文，又解码器（Deocder）去生成不定长的目标向量。

编码器将不定长向量，例如针对时许或者字符顺序不定长，转换为固定长度的向量其实也并没有太多神奇的事情，很多时候要么是按顺序叠加（类似RNN、LSTM等），要么简单的按照时间维度取某种均值也可能达到一定程度的效果。

```
包含不定长输入句子的隐含变量 H

H = encoder(X0, X1, X2, X3, ... Xn)
```

而解码器往往由个特殊的，**固定的字符输入**开始（如SOS，Start of Sentence），以固定字符输出为结束（如EOS，End of Sentence），注意，SOS是用于输入的，也就是它是要输入到解码器，而EOS是输出的，是当解码器输出它时解码器才知道自己要停了。

所以对于解码器来说，它做到的就是从固定维度的向量，生成不定长的目标句子。

我们可以将编码器的输入H，叫做H0，即最开始的隐含变量，而编码器会在这个过程中，不断修改（更新）这个隐藏变量，使得它能自我更新上下文，不会总输出一样的结果。

解码器的过程基本上等价于：

```
Y0, H1 = decoder(H0, SOS)
Y1, H2 = decoder(H1, Y0)
...
Yn, Hn-1 = decoder(Hn-2, Yn-1)
EOS, Hn = decoder(Hn-1, Yn-2)
```

### 序列到序列的应用

- 机器翻译，一种语言序列到另一种语言序列
- 语音识别，我们现将输入的连续声音讯号用某种傅立叶变换到离散的向量序列，输出文字序列
- 语音合成，我们将输入的文字序列，经过序列到序列，转换为离散的代表语音含义的离散序列，如梅尔频谱（Melspectrogram），最后经过处理为连续的语音
- 语义解析，如将自然语言转换为数据库查询语句（如SQL，SparQL），或其他非自然语言序列
- 自然语言生成，特指如开放域对话机器人、聊天机器人中的回复生成
- 文本摘要，这里特指生成式的文本摘要，即根据文章重新组织语言生成摘要，抽取式的文本摘要即抽取某个或多个句子作为摘要，可以由机器阅读理解式或分类式方法建模

## 序列标注任务（Tagging）

### 序列标注的应用

- 中文分词，输出按字的标注序列，如B代表单个字的词或者一个词的开头，I代表其他部分
- 命名实体识别，输出指定类型实体的标注，例如想要的类型实体，B作为实体开头，I作为实体其他部分，不想要的字、词用O表示
- 词性标注，标注如动词、名词等各种词性

### 延伸，机器阅读理解类任务（Machine Reading Comprehension， MRC）


- 基于MRC的命名实体识别

## 输出矩阵任务（结果为图）

- 依存句法分析

## 分类（Classification）

### 分类任务应用

- 文本分类
- 短文本分类
- 情感分析
- 情绪识别

### 融合上下文的分类

- 常识推理

### 排序任务 Learning to rank

## 只要隐藏向量的应用

- 近义词或相似词，用语言模型实现
- 语义搜索，即使用模型对句子、文章进行固定长度编码，然后使用KDTree、KBall、Annoy等方式进行搜索
- 
