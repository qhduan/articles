# GPU在哪？

人才 + 数据 + 算法 + 算力 = 落地模型

自从2006年深度学习开始展露头角，到2012年前后彻底爆发，再到现在算法已经趋于成熟（某种极限），那么有一些问题已经比较明朗了。

这里先说一下，为什么我说算法已经趋于成熟，大家看看各种NLP任务/CV任务的榜单就好，今年最新的算法，相比3年前的算法，很多提高可能只有3%，从某种意义上，一些任务已经对数据集进行了拟合。而部分成功的模型，又过于庞大，比如GPT-3，导致普通人（甚至即便是较大的公司），也没有能力负担它的具体费用，包括研究费用和使用费用。在这种情况下，我说在当前，暂时没有看到更大突破大当前，算法已经进入了某种成熟期。

当然了，这并不是悲观的看法，这种瓶颈我认为总是合理的存在。而我一直认为，在理论算法进入了某种瓶颈期的时候，也正是这些算法服务于的具体工业领域大力发挥的时候，实际上近几年我已经看到了一些非常有趣的创业公司的项目和产品。

回过头来说，如果以演绎法为基本构成的专家系统在不断的走向颓势，那么以归纳法为主的深度学习算法也才刚刚开始在各种行业开始爆发，而这种爆发需要具体的支持。

这些支持分为几个角度：

人才 + 数据 + 算法 + 算力 = 落地模型

这里面数据是相对客观的存在的，虽然专门的数据往往需要高昂的标注成本，但是除非要求特别极端，这种标注成本依然是可控的（不会比一个工程师贵）

而人力，则是可遇而不可求的，这需要整个行业都不断的向前发展，才能为社会贡献出更多的人力资源。

算法，我刚刚提到了，已经倾向于进入到某种成熟期。

所以，还剩下什么，就是算力了。（才回主题）

---

从一半的角度，算力至少包括CPU和GPU，以及部分异构运算器，例如FPGA或TPU（谷歌的那个）。

CPU的性价比在深度学习训练过程中是相当低的，这是被现实证明的。但是一些人不知道的是，CPU的预测性能往往可能还不错，从个人经验上，这主要是如果没有大批量的数据和频繁调用的话，单次调用算法，在很多模型中，CPU和内存的延迟反而很低，所以在低频应用中，CPU已经足够胜任很多工作了。

至于FPGA我只能说，我确实没有这样的经验，据我所知现在很多FPGA都是专门特化来运行某些推理器（也就是主要用来预测，或者说用来运行已经训练好的模型），而不是训练上。

TPU很美，但是它却有一些非常现实的问题：

- 首先你几乎只能用TensorFlow
- 其次你必须相对了解TPU，因为TPU的部分算子和GPU略有区别
- 需要你对谷歌的服务有所了解，比如你的数据集比较大，那么几乎只能把数据集以TFRecord的形式上传到谷歌的对象存储服务上被TPU服务器访问
- 在国内就是很麻烦

因为这些问题，所以业务上，尤其是训练上，往往我们还是会依赖更传统的选择：GPU

---

用GPU进行深度学习，那么GPU在哪？

GPU大体上可以有下面几种方式：

第一种，自己购买GPU服务器。当然了，很多时候个人要求不高，或者工作相对简单的时候，一台有独立显卡的中高端游戏笔记本，可能就已经足够完成这个工作了。当然更复杂的可能包括自己购买与安装多显卡的家用主机，或者自己配置专用的GPU服务器，甚至把这些服务器放到某个网络机房进行托管。

第二种，购买云厂商服务。现在的云业务依然在大力发展，从国内的各个云服务的促销力度也可以看得出来，所以采购它们的云服务就是主要的一种途径。或者是阿里云、腾讯云、华为云的GPU服务器，和一台普通的Linux主机没区别。

第三种，购买某些专门配置的机器学习环境，例如，微软的Azure上，就有默认给你安装了常用的TensorFlow/Jupyter/Jupyter Hub等工具的Machine Learning Service。或者是我们都知道的Google Colab，华为云等一些国内云服务厂商，也有类似的环境提供。

我们可以通过一些简单的思想实验就可以推理出上面三种的主要优缺点：

第一种优点：方便便宜；缺点：往往显存小，不能作为实际业务的推理机。

第二种优点：稳定，可以做商业服务的推理机用；缺点：贵。

第三种优点：稳定性在前面两者之间，价格往往比第二种便宜；缺点：往往只是一种实验环境，用来训练还好，部分要求不高的、或者一次性的推理服务也可以，但是要提供用户可用的对外服务就比较贵了。

---

这里我介绍一下最近我和公司在使用的第四种GPU来源：MistGPU

MistGPU是一个共享AI计算平台，提供简单易用的AI计算服务。用户可以在上面使用GPU服务器训练AI模型，按时间计费。

从价格角度来讲，相比上面的第二种乃至第三种GPU服务，MistGPU的性价比都高的惊人，而且开启与关闭服务几乎是数秒完成，因为按照时间计费，所以会把浪费的情况尽可能降低。

包括了图形界面、SSH、Jupyter Hub等多种操作方式。

开放了一个端口，可以很方便的对外提供一个服务。

默认环境安装了Jupyter Hub和TensorFlow等常用机器学习工具。

不过要说MistGPU的缺点，也是有多：

现在对外端口只有一个，所以要在一个机器上可能有种服务的话就比较麻烦。

除非是MistGPU官方特别提到的比较稳定的云服务商机器，其他机器稳定性可能没那么优秀（当然从我的体验来看已经非常好了）。

服务是基于Docker/GPU的，也就是说，现在在MistGPU的服务器上，至少你是不能再安装与使用Docker的。这给我最开始的使用的时候带来了一些麻烦，因为我的一些工具是被封装成了Docker。

排除一些缺点，如果只考虑性价比的话，MistGPU真是现在GPU训练服务中最高的。这种将大量的GPU资源拆散成碎片再零售的方法，既保证了用户快速使用，也保证了成本相对低廉。

---

最后想一想，那个公式：

人才 + 数据 + 算法 + 算力 = 落地模型

算力的问题，一定程度上已经优化到了比较高的级别：按小时计费，即用即开。

算法的问题，其实也不算太大问题，因为现在计算机领域内算法还是比较公开透明的，你要说每个公司有没有一些秘密武器，我相信有，但是没有那么严重。

数据的问题，除了明确开放的数据集，其他数据确实开放的不多。我认为这主要是因为很多数据一方面有隐私性，不能随意公开。另一方面，数据本身确实是非常有价值的，相当于一种资产，而这种资产公开了，也就没价值了。最后数据集本身公开也有成本（例如提供下载需要有带宽成本），而收获的利益又非常有限。

我想说的是，上面三个方向，都已经在短时间内很难再优化和拆解了，那么其实我好奇，人才，能被拆解吗？

一定程度上人才是一种类似算力的资源，就如同医生、律师一样，人工智能相关的人才也是一种相对明确的工种。那么它有没有可能更精确的被拆解和租赁呢？

一个更简单的类比是类似咨询公司服务，或者律师那样，有一个平台能提供人工智能专家的每小时咨询服务，结果可以是某种备忘录，或者某种具体的设计与市场调研结论。

现在市场上有部分咨询公司确实提供这种解决方案服务，不过还是太重，不知道有没有能拆的更轻量级的方法。
